{
  "cells": [
    {
      "metadata": {
        "_uuid": "844f96f3265c166dd6f824576fc06de19ebcb367"
      },
      "cell_type": "markdown",
      "source": "# Recurrent Neural Network with LSTM for Toxic Comment Classification"
    },
    {
      "metadata": {
        "_uuid": "7993d2684ece8aa61320599ba73eb4f7a0b98dba"
      },
      "cell_type": "markdown",
      "source": "# 1 - Packages\n\nLet's first import all the packages that I'll be using during this project.\n\n*  numpy is the main package for scientific computing with Python.\n*  pandas is a library for data manipulation and analysis.\n*  train_test_split is used to split our training data into training, dev datasets.\n*  Tokenizer allows to vectorize a text corpus, by turning each text into a vector.\n*  pad_sequences transforms a list of num_samples sequences into a 2D Numpy array.\n*  keras.layers contains the layers we'll use to create our RNN.\n*  Model will contain all the layers we'll use to get our output."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np, pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f4d4131a0c29027df619906d1f3c6913fcbaf2ee"
      },
      "cell_type": "markdown",
      "source": "# 2 - Dataset\n\n*  We imported the train, test data provided by MNIST.\n*  We then imported an embedding dataset which I found on kaggle datasets.\n* We then splitted the train data into two datasets, one contained features and other contained the target variables."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d21743c7a950780fbca590e69f8735fb3101e17f"
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\nembedding_file = '../input/glove6b50d/glove.6B.50d.txt'\nX_raw = train['comment_text'].values\nY = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\nX_test_raw = test['comment_text'].values",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb836d0f36f00996f4034c787bc6056504d9f3c9"
      },
      "cell_type": "code",
      "source": "embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9ab511022fd6c4377a69136b58a2ed553ede3d8"
      },
      "cell_type": "markdown",
      "source": "# 3 - Data Pre-processing\n\n*  We created an object of Tokenizer class with max numbers = 20000 and then we trained it on X_raw dataset.\n*  We then turned the list of texts into sequences.\n*  We then padded our dataset so that every sequence is about the same size.\n*  We then splitted the features, targets into training, dev sets with 1/5 ratio."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97dfd56ca32629ad31dd91dd956152896f9983b1"
      },
      "cell_type": "code",
      "source": "tokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(list(X_raw))\nX_tokenized = tokenizer.texts_to_sequences(X_raw)\nX_test_tokenized = tokenizer.texts_to_sequences(X_test_raw)\nX = pad_sequences(X_tokenized, maxlen = 100)\nX_test = pad_sequences(X_test_tokenized, maxlen = 100)\nX_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size = 0.2, random_state = 42)",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f1472d4469eb7c635d83f7e57bf4120ea5270e9"
      },
      "cell_type": "markdown",
      "source": "*  Here we converted our embedding_file into a dictionary where word is the key and the embeddin vector is the value.\n*  We then took all the embedding valued and calculated their mean and standard deviation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a343fcce07d899860d8273c402538096e7a22f15"
      },
      "cell_type": "code",
      "source": "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "(0.020940498, 0.6441043)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ca3759e5c227c9007c6f35476827877f77d71021"
      },
      "cell_type": "markdown",
      "source": "* **word_index :  ** A dictionary of words and their uniquely assigned integers.\n* **embedding_matrix:  ** A matrix which contains the relationship between different words."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab679157d5292bf43b9e8a66541942f2777c7e26"
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f835a9a4b62c1c6e68774a57341258db907f770"
      },
      "cell_type": "markdown",
      "source": "# 4 - Model\n\nNow as we have prepared our training data, we can start building our model.\n\n**In this model we have used - **\n\n* **Embedding layer :  ** Turns positive integers (indexes) into dense vectors of fixed size.\n\n* **Bi-directional RNN with LSTM :  ** It involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second.\n\n* **Dense :  ** A simple Neural Network layer.\n\n* **Dropout :  ** So that the weights don't rely too much on some activations.\n\n* **Dense:  ** Output layer with sogmoid as the activation function.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f55a0d13bb2cd323fe10377ce87459dac4e47040"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b5b8d6b913f4f4c911faee73dc35d7a0d33a7a9a"
      },
      "cell_type": "markdown",
      "source": "# 5 - Train the model\n\nAs our model is now ready we can start it's training."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "145b80b6b900eb2b14773e9a4ae73fd5e29c9d87"
      },
      "cell_type": "code",
      "source": "model.fit(X_train, Y_train, batch_size=32, epochs=2, validation_split=0.1);",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 114890 samples, validate on 12766 samples\nEpoch 1/2\n114890/114890 [==============================] - 619s 5ms/step - loss: 0.2830 - acc: 0.9876 - val_loss: 0.2917 - val_acc: 0.9896\nEpoch 2/2\n114890/114890 [==============================] - 616s 5ms/step - loss: 0.2673 - acc: 0.9908 - val_loss: 0.2884 - val_acc: 0.9937\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1c8c54f5e46df39cc4ef37391df0898c657d1385"
      },
      "cell_type": "markdown",
      "source": "# 6 - Test the model\n\nNow that our model is trained we can test how well our model can perform on unseen datasets"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65d992e6a6d0a5070c8589ddbde6c3068be81100"
      },
      "cell_type": "code",
      "source": "dev_loss, dev_accuracy = model.evaluate(X_dev, Y_dev)\nprint('loss on dev set is: {}'.format(round(dev_loss, 2)))\nprint('accuracy on dev set is: {}'.format(round(dev_accuracy, 2)))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "31915/31915 [==============================] - 28s 888us/step\nloss on dev set is: 0.27\naccuracy on dev set is: 0.99\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "23d7a7c200fc28af4cb774cadaf53dc79b758385"
      },
      "cell_type": "markdown",
      "source": "# 7 - Submit the result"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed1849f71560b05f4025a44e3e8a28339f31ef71"
      },
      "cell_type": "code",
      "source": "y_test = model.predict([X_test], batch_size=1024, verbose=1)\nsample_submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsample_submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_test\nsample_submission.to_csv('submission.csv', index=False)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "153164/153164 [==============================] - 79s 513us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b537c44aa816df01f2f3af35dbb0ee73eaf2961"
      },
      "cell_type": "markdown",
      "source": "# Conclusion\n* In this project I learned how to use keras api.\n* I learned a lot of interesting stuff like how to convert a sentence into a form which can be used by the model and the inner working of the lstm(Which I didn't use bcz I used keras which handles all that confusing stuff itself).\n* In this project I learned how to handel the text datasets and how to perform multiclass classification for cases like this."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}